version: 0.0.1
title: Apache Kafka
description: Log parser for Apache Kafka
parameters:
  server_log_path:
    label: Server Log Path
    description: Apache Kafka server log path
    type: string
    required: true
  controller_log_path:
    label: Controller Log Path
    description: Apache Kafka controller log path
    type: string
  state_change_log_path:
    label: State Change Log Path
    description: Apache Kafka state-change log path
    type: string
  log_cleaner_log_path:
    label: Log Cleaner Log Path
    description: Apache Kafka log-cleaner log path
    type: string
  start_at:
    label: Start At
    description:  Start reading file from 'beginning' or 'end'
    type: enum
    valid_values:
      - beginning
      - end
    default: end
pipeline:
  # Use file input for all log files as it uses the same regex parser.
  # {{ if .server_log_path }}
  - id: log_reader
    type: file_input
    include:
      - {{ .server_log_path}}
  # Only use paths if the parameter was defined in the pipeline
{{ if .controller_log_path }}      - {{ .controller_log_path}}{{ end }}
{{ if .state_change_log_path }}      - {{ .state_change_log_path}}{{ end }}
{{ if .log_cleaner_log_path }}      - {{ .log_cleaner_log_path}}{{ end }}
    multiline:
      line_start_pattern: '\[\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}\]'
    file_name_field: log_file_name
    write_to: log_entry
    start_at: {{ or .start_at "end" }}

  # Parse log entries.
  - id: server_regex_parser
    type: regex_parser
    parse_from: log_entry
    regex: '^\[(?P<date_time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}),(?P<milliseconds>\d{3})\] (?P<severity>[^ ]+) (?P<message>.*)'
    severity:
      parse_from: severity
      mapping:
        warning: warn
        critical: fatal

  # Determine what log type the log entry is and route to appropriate metadata plugin
  - id: filename_router
    type: router
    routes:
      - expr: '$record.log_file_name != nil and $record.log_file_name matches "^server"'
        output: add_server_log_name
      - expr: '$record.log_file_name != nil and $record.log_file_name matches "^controller"'
        output: add_controller_log_name
      - expr: '$record.log_file_name != nil and $record.log_file_name matches "^state-change"'
        output: add_state_change_log_name
      - expr: '$record.log_file_name != nil and $record.log_file_name matches "^log-cleaner"'
        output: add_log_cleaner_log_name
      - expr: true
        output: default_log_name

  # Add log_name label
  - id: add_server_log_name
    type: metadata
    labels:
      log_name: 'kafka.server'
    output: timestamp_restructurer

  - id: add_controller_log_name
    type: metadata
    labels:
      log_name: 'kafka.controller'
    output: timestamp_restructurer

  - id: add_state_change_log_name
    type: metadata
    labels:
      log_name: 'kafka.state_change'
    output: timestamp_restructurer

  - id: add_log_cleaner_log_name
    type: metadata
    labels:
      log_name: 'kafka.log_cleaner'
    output: timestamp_restructurer

  # Use this label if unable to tell the type of log from log file name.
  - id: default_log_name
    type: metadata
    labels:
      log_name: 'kafka'
    output: timestamp_restructurer

  # Due to limitations in golang time we cannot have a comma in our timestamp we split timestamp into two fields 'date_time' and 'milliseconds'.
  # This will combine and remove 'date_time' and 'milliseconds' fields into a timestamp field.
  - id: timestamp_restructurer
    type: restructure
    ops:
      - add:
          field: timestamp
          value_expr: '$record.date_time + "." + $record.milliseconds'
      - remove: date_time
      - remove: milliseconds

  # Parse timestamp and exit plugin
  - type: time_parser
    parse_from: timestamp
    layout: '%F %T.%L'
    output: {{ .output }}
  # {{ end }}
